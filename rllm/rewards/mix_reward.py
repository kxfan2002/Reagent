"""
æ··åˆ Reward å‡½æ•°ï¼šè‡ªåŠ¨æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©æ•°å­¦åˆ¤åˆ†æˆ–æ–‡æœ¬åˆ¤åˆ†

åŸºäºåŸå§‹å®ç°ï¼š
- æ•°å­¦åˆ¤åˆ†ï¼šä½¿ç”¨ MathRuler çš„ grade_answer
- æ–‡æœ¬åˆ¤åˆ†ï¼šä½¿ç”¨ search_reward çš„ F1/EM è¯„ä¼°
"""

import re
from typing import Any

from rllm.rewards.math_reward import RewardMathFn
from rllm.rewards.reward_types import RewardConfig, RewardInput, RewardOutput
from rllm.rewards.search_reward import RewardSearchFn


class RewardMixFn:
    """
    æ··åˆ Reward å‡½æ•°ç±»ï¼Œèƒ½å¤Ÿæ ¹æ®ç­”æ¡ˆç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•
    
    - æ•°å­¦ç­”æ¡ˆï¼ˆæ•´æ•°ã€å°æ•°ã€åˆ†æ•°ã€LaTeXè¡¨è¾¾å¼ï¼‰â†’ ä½¿ç”¨ RewardMathFn
    - æ–‡æœ¬ç­”æ¡ˆ â†’ ä½¿ç”¨ RewardSearchFn (F1/EM)
    """
    
    def __init__(self, config: RewardConfig):
        self.config = config
        self.math_reward_fn = RewardMathFn(config)
        self.search_reward_fn = RewardSearchFn(config)
    
    def extract_boxed_content(self, text: str) -> str | None:
        """
        ä» \\boxed{} ä¸­æå–å†…å®¹ï¼Œæ”¯æŒåµŒå¥—æ‹¬å·
        
        Args:
            text: åŒ…å« \\boxed{} çš„æ–‡æœ¬
            
        Returns:
            æå–çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        if not text or not isinstance(text, str):
            return None
        
        # å¯»æ‰¾æœ€åä¸€ä¸ª \boxed{ çš„ä½ç½®
        box_match = re.search(r'\\boxed?\{', text)
        if box_match:
            start_pos = box_match.end() - 1  # æŒ‡å‘å¼€å§‹çš„ {
            brace_count = 0
            content_start = box_match.end()
            
            for i, char in enumerate(text[start_pos:], start_pos):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        content = text[content_start:i].strip()
                        return content if content else None
        return None
    
    def is_math_answer(self, answer: str) -> bool:
        """
        åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸ºæ•°å­¦ç±»å‹
        
        è¯†åˆ«è§„åˆ™ï¼š
        - çº¯æ•°å­—ï¼ˆæ•´æ•°ã€å°æ•°ã€åˆ†æ•°ï¼‰
        - LaTeX æ•°å­¦è¡¨è¾¾å¼
        - ç§‘å­¦è®¡æ•°æ³•
        
        Args:
            answer: å¾…åˆ¤æ–­çš„ç­”æ¡ˆå­—ç¬¦ä¸²
            
        Returns:
            True è¡¨ç¤ºæ•°å­¦ç­”æ¡ˆï¼ŒFalse è¡¨ç¤ºæ–‡æœ¬ç­”æ¡ˆ
        """
        if not answer or not isinstance(answer, str):
            return False
        
        answer = answer.strip()
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—ï¼ˆæ•´æ•°ï¼‰
        if re.match(r'^[+-]?\d+$', answer):
            return True
        
        # 2. æ£€æŸ¥æ˜¯å¦ä¸ºçº¯å°æ•°
        if re.match(r'^[+-]?\d*\.\d+$', answer):
            return True
        
        # 3. æ£€æŸ¥æ˜¯å¦ä¸ºçº¯åˆ†æ•°
        if re.match(r'^[+-]?\d+/\d+$', answer):
            return True
        
        # 4. æ£€æŸ¥ LaTeX æ•°å­¦è¡¨è¾¾å¼
        latex_patterns = [
            r'\\boxed\{.*\}',                    # LaTeX boxed
            r'\\[dtf]?frac\{.*\}\{.*\}',         # LaTeX åˆ†æ•° (\frac, \dfrac, \tfrac)
            r'^\$.*\$$',                          # LaTeX æ•°å­¦æ¨¡å¼
            r'[+\-*/=<>â‰¤â‰¥â‰ âˆ«âˆ‘âˆ]',                # æ•°å­¦è¿ç®—ç¬¦
        ]
        
        for pattern in latex_patterns:
            if re.search(pattern, answer):
                return True
        
        # 5. ç§‘å­¦è®¡æ•°æ³•
        if re.match(r'^[+-]?\d*\.?\d+[eE][+-]?\d+$', answer):
            return True
        
        # 6. åŒ…å«æ•°å­¦å˜é‡å’Œè¿ç®—çš„è¡¨è¾¾å¼ï¼ˆå¦‚ 2x+3, x^2ï¼‰
        if re.search(r'[a-zA-Z]\s*[\^+\-*/]|[\^+\-*/]\s*[a-zA-Z]', answer):
            return True
        
        return False
    
    def __call__(self, task_info: dict, action: str) -> RewardOutput:
        """
        è®¡ç®—æ··åˆ rewardï¼Œè‡ªåŠ¨æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©è¯„ä¼°æ–¹æ³•
        
        Args:
            task_info: ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« ground_truth ç­‰
            action: æ¨¡å‹çš„å›ç­”
            
        Returns:
            RewardOutput: åŒ…å« rewardã€is_correct å’Œ metadata
        """
        if not action or action == "":
            return RewardOutput(
                reward=self.config.format_error_reward,
                is_correct=False,
                metadata={"error": "Empty action", "evaluation_method": "none"}
            )
        
        # è·å– ground_truth
        ground_truth = task_info.get("ground_truth") or task_info.get("answer")
        if ground_truth is None:
            return RewardOutput(
                reward=self.config.unk_error_reward,
                is_correct=False,
                metadata={"error": "No ground truth provided", "evaluation_method": "none"}
            )
        
        # æå–ç­”æ¡ˆå†…å®¹ï¼ˆä» \boxed{} ä¸­ï¼‰
        extracted_answer = self.extract_boxed_content(action)
        if extracted_answer is None:
            # å¦‚æœæ²¡æœ‰ \boxed{}ï¼Œä½¿ç”¨ search_reward çš„æå–æ–¹æ³•
            extracted_answer = self.search_reward_fn.extract_answer_from_response(action)
        
        # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°ç­”æ¡ˆå’Œæ ‡å‡†ç­”æ¡ˆ
        # print(f"[RewardMixFn] æå–çš„ç­”æ¡ˆ: {extracted_answer}", flush=True)
        # print(f"[RewardMixFn] æ ‡å‡†ç­”æ¡ˆ: {ground_truth}", flush=True)
        # print(f"[RewardMixFn] å®Œæ•´å›ç­”: {action[:200]}...", flush=True)
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šyes/no ç­”æ¡ˆï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        # å¦‚æœ ground_truth æ˜¯ yes æˆ– noï¼Œåªè¦æå–çš„ç­”æ¡ˆä¸­åŒ…å«å¯¹åº”çš„è¯å°±ç®—æ­£ç¡®
        gt_normalized = str(ground_truth).strip().lower()
        if gt_normalized in ["yes", "no"]:
            extracted_normalized = str(extracted_answer).strip().lower() if extracted_answer else ""
            # æ£€æŸ¥æå–çš„ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å« ground_truth
            if gt_normalized in extracted_normalized:
                return RewardOutput(
                    reward=self.config.correct_reward,
                    is_correct=True,
                    metadata={
                        "evaluation_method": "yes_no_match",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "yes_no"
                    }
                )
            else:
                return RewardOutput(
                    reward=self.config.incorrect_reward,
                    is_correct=False,
                    metadata={
                        "evaluation_method": "yes_no_mismatch",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "yes_no"
                    }
                )
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼štrue/false ç­”æ¡ˆï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        # å¦‚æœ ground_truth æ˜¯ true æˆ– falseï¼Œåªè¦æå–çš„ç­”æ¡ˆä¸­åŒ…å«å¯¹åº”çš„è¯å°±ç®—æ­£ç¡®
        if gt_normalized in ["true", "false"]:
            extracted_normalized = str(extracted_answer).strip().lower() if extracted_answer else ""
            # æ£€æŸ¥æå–çš„ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å« ground_truth
            if gt_normalized in extracted_normalized:
                return RewardOutput(
                    reward=self.config.correct_reward,
                    is_correct=True,
                    metadata={
                        "evaluation_method": "true_false_match",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "true_false"
                    }
                )
            else:
                return RewardOutput(
                    reward=self.config.incorrect_reward,
                    is_correct=False,
                    metadata={
                        "evaluation_method": "true_false_mismatch",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "true_false"
                    }
                )
        
        # åˆ¤æ–­ç­”æ¡ˆç±»å‹
        # è½¬æ¢ ground_truth ä¸ºå­—ç¬¦ä¸²è¿›è¡Œåˆ¤æ–­
        if isinstance(ground_truth, list):
            # å¦‚æœ ground_truth æ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ 
            gt_sample = str(ground_truth[0]) if ground_truth else ""
        else:
            gt_sample = str(ground_truth)
        
        is_math = self.is_math_answer(extracted_answer) or self.is_math_answer(gt_sample)
        
        # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°ç­”æ¡ˆç±»å‹åˆ¤æ–­
        # print(f"[RewardMixFn] ç­”æ¡ˆç±»å‹åˆ¤æ–­: is_math={is_math}, extracted_answer={extracted_answer}, gt_sample={gt_sample}", flush=True)
        
        # æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©è¯„ä¼°æ–¹æ³•
        if is_math:
            # ä½¿ç”¨æ•°å­¦ reward å‡½æ•°
            try:
                result = self.math_reward_fn(task_info, action)
                result.metadata["evaluation_method"] = "math"
                result.metadata["extracted_answer"] = extracted_answer
                result.metadata["answer_type"] = "math"
                
                # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°æ•°å­¦åˆ¤åˆ†ç»“æœ
                print(f"[RewardMixFn] ä½¿ç”¨æ•°å­¦è¯„ä¼° -> reward={result.reward}, is_correct={result.is_correct}", flush=True)
                
                return result
            except Exception as e:
                # å¦‚æœæ•°å­¦è¯„ä¼°å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æœ¬è¯„ä¼°
                print(f"âš ï¸  Math evaluation failed: {e}, falling back to text evaluation", flush=True)
                pass
        
        # ä½¿ç”¨æ–‡æœ¬ reward å‡½æ•°ï¼ˆsearch rewardï¼‰
        reward_input = RewardInput(task_info=task_info, action=action)
        result = self.search_reward_fn(reward_input)
        result.metadata["evaluation_method"] = "text"
        result.metadata["answer_type"] = "text"
        
        # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°æ–‡æœ¬åˆ¤åˆ†ç»“æœ
        print(f"[RewardMixFn] ä½¿ç”¨æ–‡æœ¬è¯„ä¼° -> reward={result.reward}, is_correct={result.is_correct}", flush=True)
        
        return result


def mixed_reward_fn(task_info: dict, action: str) -> RewardOutput:
    """
    ä¾¿æ·çš„æ··åˆ reward å‡½æ•°æ¥å£
    
    è‡ªåŠ¨æ ¹æ®ç­”æ¡ˆç±»å‹é€‰æ‹©åˆé€‚çš„åˆ¤åˆ†å‡½æ•°ï¼š
    - æ•°å­¦ç­”æ¡ˆ â†’ ä½¿ç”¨ math_acc_reward (åŸºäº MathRuler)
    - æ–‡æœ¬ç­”æ¡ˆ â†’ ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦ (F1/EM)
    
    Args:
        task_info: ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« ground_truth ç­‰
        action: æ¨¡å‹çš„å›ç­”
        
    Returns:
        RewardOutput: åŒ…å« rewardã€is_correct å’Œ metadata
        
    Example:
        >>> task_info = {"ground_truth": "42"}
        >>> action = "The answer is \\boxed{42}"
        >>> result = mixed_reward_fn(task_info, action)
        >>> print(result.reward)  # 1.0
        >>> print(result.metadata["evaluation_method"])  # "math"
    """
    reward_config = RewardConfig()
    reward_fn = RewardMixFn(reward_config)
    return reward_fn(task_info, action)


def adaptive_reward_fn(task_info: dict, action: str) -> RewardOutput:
    """
    è‡ªé€‚åº” reward å‡½æ•°ï¼Œæ ¹æ® data_source å­—æ®µè‡ªåŠ¨é€‰æ‹©è¯„ä¼°æ–¹æ³•
    
    å¦‚æœ task_info ä¸­æœ‰ data_source å­—æ®µï¼š
    - data_source == "math" â†’ å¼ºåˆ¶ä½¿ç”¨æ•°å­¦è¯„ä¼°
    - data_source == "search" â†’ å¼ºåˆ¶ä½¿ç”¨æ–‡æœ¬è¯„ä¼°
    - å…¶ä»– â†’ è‡ªåŠ¨åˆ¤æ–­ç­”æ¡ˆç±»å‹
    
    Args:
        task_info: ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼Œå¯åŒ…å« data_source å­—æ®µ
        action: æ¨¡å‹çš„å›ç­”
        
    Returns:
        RewardOutput: åŒ…å« rewardã€is_correct å’Œ metadata
    """
    reward_config = RewardConfig()
    data_source = task_info.get("data_source", "auto")
    meta_info_source = task_info.get("meta_info_source", "auto")
    category = task_info.get("category", "auto")
    # import pdb; pdb.set_trace()
    ground_truth = task_info.get("ground_truth") or task_info.get("answer")
    extracted_answer = None
    mix_fn = RewardMixFn(reward_config)
    if ground_truth is not None:
        # éœ€è¦å®ä¾‹åŒ–ä¸€ä¸ª RewardMixFn æ¥ä½¿ç”¨å…¶ extract_boxed_content æ–¹æ³•
        # mix_fn = RewardMixFn(reward_config)
        extracted_answer = mix_fn.extract_boxed_content(action)
        if extracted_answer is None:
            # å¦‚æœæ²¡æœ‰ \boxed{}ï¼Œä½¿ç”¨ search_reward çš„æå–æ–¹æ³•
            search_fn = RewardSearchFn(reward_config)
            extracted_answer = search_fn.extract_answer_from_response(action)
        
        gt_normalized = str(ground_truth).strip().lower()
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šyes/no ç­”æ¡ˆ
        if gt_normalized in ["yes", "no"]:
            extracted_normalized = str(extracted_answer).strip().lower() if extracted_answer else ""
            if gt_normalized in extracted_normalized:
                print(f"[adaptive_reward_fn] ä½¿ç”¨ yes/no è¯„ä¼° -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={reward_config.correct_reward}", flush=True)
                return RewardOutput(
                    reward=reward_config.correct_reward,
                    is_correct=True,
                    metadata={
                        "evaluation_method": "yes_no_match",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "yes_no",
                        "data_source": data_source
                    }
                )
            else:
                print(f"[adaptive_reward_fn] ä½¿ç”¨ yes/no è¯„ä¼° -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={reward_config.incorrect_reward}", flush=True)
                return RewardOutput(
                    reward=reward_config.incorrect_reward,
                    is_correct=False,
                    metadata={
                        "evaluation_method": "yes_no_mismatch",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "yes_no",
                        "data_source": data_source
                    }
                )
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼štrue/false ç­”æ¡ˆ
        if gt_normalized in ["true", "false"]:
            extracted_normalized = str(extracted_answer).strip().lower() if extracted_answer else ""
            if gt_normalized in extracted_normalized:
                print(f"[adaptive_reward_fn] ä½¿ç”¨ true/false è¯„ä¼° -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={reward_config.correct_reward}", flush=True)
                return RewardOutput(
                    reward=reward_config.correct_reward,
                    is_correct=True,
                    metadata={
                        "evaluation_method": "true_false_match",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "true_false",
                        "data_source": data_source
                    }
                )
            else:
                print(f"[adaptive_reward_fn] ä½¿ç”¨ true/false è¯„ä¼° -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={reward_config.incorrect_reward}", flush=True)
                return RewardOutput(
                    reward=reward_config.incorrect_reward,
                    is_correct=False,
                    metadata={
                        "evaluation_method": "true_false_mismatch",
                        "extracted_answer": extracted_answer,
                        "ground_truth": ground_truth,
                        "answer_type": "true_false",
                        "data_source": data_source
                    }
                )
    
    if meta_info_source == "math" or category == "math" or data_source == "math":
        # å¼ºåˆ¶ä½¿ç”¨æ•°å­¦è¯„ä¼°
        # print(f"[adaptive_reward_fn] ä½¿ç”¨æ•°å­¦è¯„ä¼°ï¼ˆå¼ºåˆ¶ï¼‰", flush=True)
        math_fn = RewardMathFn(reward_config)
        result = math_fn(task_info, action)
        result.metadata["evaluation_method"] = "math"
        result.metadata["data_source"] = data_source
        print(f"[adaptive_reward_fn] ä½¿ç”¨æ•°å­¦è¯„ä¼°ï¼ˆå¼ºåˆ¶ï¼‰ -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={result.reward}", flush=True)
        
        # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°ç»“æœ
        # print(f"[adaptive_reward_fn] æ•°å­¦è¯„ä¼°ç»“æœ -> reward={result.reward}, is_correct={result.is_correct}", flush=True)
        
        return result
    elif data_source == "search" or str(data_source).lower().find("qa") != -1:
        # å¼ºåˆ¶ä½¿ç”¨æ–‡æœ¬è¯„ä¼°
        # print(f"[adaptive_reward_fn] ä½¿ç”¨æ–‡æœ¬è¯„ä¼°ï¼ˆå¼ºåˆ¶ï¼‰", flush=True)
        search_fn = RewardSearchFn(reward_config)
        reward_input = RewardInput(task_info=task_info, action=action)
        result = search_fn(reward_input)
        result.metadata["evaluation_method"] = "text"
        result.metadata["data_source"] = data_source
        print(f"[adaptive_reward_fn] ä½¿ç”¨æ–‡æœ¬è¯„ä¼°ï¼ˆå¼ºåˆ¶ï¼‰ -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={result.reward}", flush=True)
        # ğŸ”¥ Debug æ—¥å¿—ï¼šæ‰“å°ç»“æœ
        # print(f"[adaptive_reward_fn] æ–‡æœ¬è¯„ä¼°ç»“æœ -> reward={result.reward}, is_correct={result.is_correct}", flush=True)
        
        return result
    else:
        # è‡ªåŠ¨åˆ¤æ–­
        # print(f"[adaptive_reward_fn] ä½¿ç”¨è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼", flush=True)
        # mix_fn = RewardMixFn(reward_config)
        result = mix_fn(task_info, action)
        result.metadata["data_source"] = data_source
        print(f"[adaptive_reward_fn] ä½¿ç”¨è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼ -> ground_truth={ground_truth} extracted_answer={extracted_answer} reward={result.reward}", flush=True)
        return result


if __name__ == "__main__":
    # æµ‹è¯•æ•°å­¦ç­”æ¡ˆ
    print("=" * 80)
    print("æµ‹è¯• 1: æ•°å­¦ç­”æ¡ˆ")
    print("=" * 80)
    task_info_math = {
        "ground_truth": "42",
        "data_source": "math"
    }
    action_math = "After calculation, the answer is \\boxed{42}"
    result_math = mixed_reward_fn(task_info_math, action_math)
    print(f"Reward: {result_math.reward}")
    print(f"Is Correct: {result_math.is_correct}")
    print(f"Metadata: {result_math.metadata}")
    print()
    
    # æµ‹è¯•æ–‡æœ¬ç­”æ¡ˆ
    print("=" * 80)
    print("æµ‹è¯• 2: æ–‡æœ¬ç­”æ¡ˆ")
    print("=" * 80)
    task_info_text = {
        "ground_truth": "Paris",
        "data_source": "search"
    }
    action_text = "The capital of France is \\boxed{Paris}"
    result_text = mixed_reward_fn(task_info_text, action_text)
    print(f"Reward: {result_text.reward}")
    print(f"Is Correct: {result_text.is_correct}")
    print(f"Metadata: {result_text.metadata}")
    print()
    
    # æµ‹è¯•è‡ªé€‚åº”æ¨¡å¼
    print("=" * 80)
    print("æµ‹è¯• 3: è‡ªé€‚åº”æ¨¡å¼ï¼ˆè‡ªåŠ¨åˆ¤æ–­ï¼‰")
    print("=" * 80)
    task_info_auto = {
        "ground_truth": "9:45"
    }
    action_auto = "\\boxed{9:45}"
    result_auto = adaptive_reward_fn(task_info_auto, action_auto)
    print(f"Reward: {result_auto.reward}")
    print(f"Is Correct: {result_auto.is_correct}")
    print(f"Metadata: {result_auto.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• yes/no ç­”æ¡ˆï¼ˆæ­£ç¡®æƒ…å†µï¼‰
    print("=" * 80)
    print("æµ‹è¯• 4: Yes/No ç­”æ¡ˆ - æ­£ç¡®ï¼ˆåŒ…å«é¢å¤–æ–‡æœ¬ï¼‰")
    print("=" * 80)
    task_info_yes = {
        "ground_truth": "yes"
    }
    action_yes = "\\boxed{Yes, the reason is that the evidence clearly supports this conclusion.}"
    result_yes = mixed_reward_fn(task_info_yes, action_yes)
    print(f"Reward: {result_yes.reward}")
    print(f"Is Correct: {result_yes.is_correct}")
    print(f"Metadata: {result_yes.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• yes/no ç­”æ¡ˆï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    print("=" * 80)
    print("æµ‹è¯• 5: Yes/No ç­”æ¡ˆ - å¤§å°å†™ä¸æ•æ„Ÿ")
    print("=" * 80)
    task_info_no = {
        "ground_truth": "NO"
    }
    action_no = "\\boxed{no, because the conditions are not met}"
    result_no = mixed_reward_fn(task_info_no, action_no)
    print(f"Reward: {result_no.reward}")
    print(f"Is Correct: {result_no.is_correct}")
    print(f"Metadata: {result_no.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• yes/no ç­”æ¡ˆï¼ˆé”™è¯¯æƒ…å†µï¼‰
    print("=" * 80)
    print("æµ‹è¯• 6: Yes/No ç­”æ¡ˆ - é”™è¯¯ï¼ˆä¸åŒ¹é…ï¼‰")
    print("=" * 80)
    task_info_yes_wrong = {
        "ground_truth": "yes"
    }
    action_yes_wrong = "\\boxed{No, this is incorrect}"
    result_yes_wrong = mixed_reward_fn(task_info_yes_wrong, action_yes_wrong)
    print(f"Reward: {result_yes_wrong.reward}")
    print(f"Is Correct: {result_yes_wrong.is_correct}")
    print(f"Metadata: {result_yes_wrong.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• true/false ç­”æ¡ˆï¼ˆæ­£ç¡®æƒ…å†µï¼‰
    print("=" * 80)
    print("æµ‹è¯• 7: True/False ç­”æ¡ˆ - æ­£ç¡®ï¼ˆåŒ…å«é¢å¤–æ–‡æœ¬ï¼‰")
    print("=" * 80)
    task_info_true = {
        "ground_truth": "true"
    }
    action_true = "\\boxed{True, because the statement is logically valid.}"
    result_true = mixed_reward_fn(task_info_true, action_true)
    print(f"Reward: {result_true.reward}")
    print(f"Is Correct: {result_true.is_correct}")
    print(f"Metadata: {result_true.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• true/false ç­”æ¡ˆï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    print("=" * 80)
    print("æµ‹è¯• 8: True/False ç­”æ¡ˆ - å¤§å°å†™ä¸æ•æ„Ÿ")
    print("=" * 80)
    task_info_false = {
        "ground_truth": "FALSE"
    }
    action_false = "\\boxed{false, the condition does not hold}"
    result_false = mixed_reward_fn(task_info_false, action_false)
    print(f"Reward: {result_false.reward}")
    print(f"Is Correct: {result_false.is_correct}")
    print(f"Metadata: {result_false.metadata}")
    print()
    
    # ğŸ”¥ æµ‹è¯• true/false ç­”æ¡ˆï¼ˆé”™è¯¯æƒ…å†µï¼‰
    print("=" * 80)
    print("æµ‹è¯• 9: True/False ç­”æ¡ˆ - é”™è¯¯ï¼ˆä¸åŒ¹é…ï¼‰")
    print("=" * 80)
    task_info_true_wrong = {
        "ground_truth": "true"
    }
    action_true_wrong = "\\boxed{False, this is incorrect}"
    result_true_wrong = mixed_reward_fn(task_info_true_wrong, action_true_wrong)
    print(f"Reward: {result_true_wrong.reward}")
    print(f"Is Correct: {result_true_wrong.is_correct}")
    print(f"Metadata: {result_true_wrong.metadata}")
    print()
    
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

